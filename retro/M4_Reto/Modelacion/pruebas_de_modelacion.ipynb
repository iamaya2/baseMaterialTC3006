{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:57:15.273681900Z",
     "start_time": "2023-08-28T00:57:15.257775800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/Github/Reto-3006C-equipo5/retro/M4_Reto/Data/train_clean.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:55:02.683698800Z",
     "start_time": "2023-08-28T00:55:02.675692300Z"
    }
   },
   "id": "8255e3580eb4d715"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "((708, 15), (178, 15))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(\"Survived\", axis=1)\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:55:02.858546Z",
     "start_time": "2023-08-28T00:55:02.838527400Z"
    }
   },
   "id": "ae39ca6093908eb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cbbdc727e579352"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.764\n",
      "Random Forest: 0.986\n"
     ]
    }
   ],
   "source": [
    "# Haremos una prueba con un random forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, y_pred)\n",
    "training_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "\n",
    "print('Random Forest: {:.3f}'.format(testing_accuracy))\n",
    "print('Random Forest: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:01:26.983034400Z",
     "start_time": "2023-08-28T01:01:26.830895100Z"
    }
   },
   "id": "7769f58273ef014f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta prueba dado que el modelo esta teniendo muchos mejores resultados en el training que en el testing, podemos concluir que esta haciendo overfitting por lo que intentaremos utilizar un modelo con regularizacion para ver si podemos mejorar los resultados"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "422be624b417e2b2"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Random Forest: 0.809\n",
      "Random Forest: 0.893\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],      \n",
    "    'max_depth': [None, 10, 20, 30],      \n",
    "    'min_samples_split': [2, 5, 10],      \n",
    "    'min_samples_leaf': [1, 2, 4],        \n",
    "    'bootstrap': [True, False],          \n",
    "    'criterion': ['gini', 'entropy'] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',       \n",
    "    cv=5,                     \n",
    "    verbose=1,               \n",
    "    n_jobs=-1                   \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "training_accuracy = accuracy_score(y_train, best_clf.predict(X_train))\n",
    "\n",
    "print('Random Forest: {:.3f}'.format(testing_accuracy))\n",
    "print('Random Forest: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:17:10.249057600Z",
     "start_time": "2023-08-28T01:16:28.965592600Z"
    }
   },
   "id": "49b3b24285e92041"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Despues de utilizar la optimizacion de parametros podemos ver que los resultados son mucho mejores, sin embargo, el modelo sigue haciendo overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36dbb1b44c1f4104"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f27f809003b8c5fd"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.775\n",
      "Logistic Regression: 0.847\n"
     ]
    }
   ],
   "source": [
    "# Haremos una prueba con un logistic regression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, y_pred)\n",
    "training_accuracy = accuracy_score(y_train, lr.predict(X_train))\n",
    "\n",
    "print('Logistic Regression: {:.3f}'.format(testing_accuracy))\n",
    "print('Logistic Regression: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:23:51.345997300Z",
     "start_time": "2023-08-28T01:23:51.316977400Z"
    }
   },
   "id": "68bfd7c717bc0529"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta prueba dado que el modelo esta teniendo muchos mejores resultados en el training que en el testing, podemos concluir que esta haciendo overfitting por lo que intentaremos utilizar un modelo con regularizacion para ver si podemos mejorar los resultados"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b4bc0070bde4a0d"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "{'C': 2, 'l1_ratio': 0.5, 'max_iter': 5000, 'penalty': 'elasticnet'}\n",
      "Logistic Regression: 0.775\n",
      "Logistic Regression: 0.833\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='saga')\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],     \n",
    "    'C': [0.001, 0.01, 0.1, 1, 2, 5, 10, 100],          \n",
    "    'l1_ratio': [0, 0.1, 0.2, 0.5, 1],\n",
    "    'max_iter': [500, 1000, 2000, 3000, 5000, 10000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',         \n",
    "    cv=5,                       \n",
    "    verbose=1,                 \n",
    "    n_jobs=-1                   \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "training_accuracy = accuracy_score(y_train, best_clf.predict(X_train))\n",
    "\n",
    "print('Logistic Regression: {:.3f}'.format(testing_accuracy))\n",
    "print('Logistic Regression: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:23:23.653564400Z",
     "start_time": "2023-08-28T01:21:41.707374600Z"
    }
   },
   "id": "a06e4fb172b0a634"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora despues de aplicar regularizacion podemos ver que los resultados son mucho mejores."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c7e5fd97a44c48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "536c585b03239f62"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.787\n",
      "XGBoost: 0.973\n"
     ]
    }
   ],
   "source": [
    "# Haremos una prueba con un xgboost\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, y_pred)\n",
    "training_accuracy = accuracy_score(y_train, xgb.predict(X_train))\n",
    "\n",
    "print('XGBoost: {:.3f}'.format(testing_accuracy))\n",
    "print('XGBoost: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:01:42.260527Z",
     "start_time": "2023-08-28T01:01:42.196469400Z"
    }
   },
   "id": "901d9a43ed01a029"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta prueba dado que el modelo esta teniendo muchos mejores resultados en el training que en el testing, podemos concluir que esta haciendo overfitting por lo que intentaremos utilizar grid search para optimizar los parametros del modelo y ver si podemos mejorar los resultados"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7a959184b2c5dba"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5040 candidates, totalling 25200 fits\n",
      "{'alpha': 0.5, 'gamma': 0.5, 'lambda': 0.1, 'max_depth': 2, 'min_child_weight': 1}\n",
      "XGBoost: 0.758\n",
      "XGBoost: 0.884\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.2, 0.5, 1, 2, 10],             \n",
    "    'lambda': [0.05, 0.1, 0.2, 0.5, 1, 2, 10],            \n",
    "    'gamma': [0, 0.1, 0.5, 1],             \n",
    "    'max_depth': [1, 2, 3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 5, 10, 15, 20]   \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,                     \n",
    "    verbose=1,            \n",
    "    n_jobs=-1        \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, best_clf.predict(X_test))\n",
    "training_accuracy = accuracy_score(y_train, best_clf.predict(X_train))\n",
    "\n",
    "print('XGBoost: {:.3f}'.format(testing_accuracy))\n",
    "print('XGBoost: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:13:16.922629500Z",
     "start_time": "2023-08-28T01:11:27.010606700Z"
    }
   },
   "id": "78c88dfa13181507"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Despues de utilizar la optimizacion de parametros podemos ver que los resultados son mucho mejores, sin embargo, el modelo sigue haciendo overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ff8a05f581db3be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Voting Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3d43a31d21edb76"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.764\n",
      "Voting Classifier: 0.879\n"
     ]
    }
   ],
   "source": [
    "# Se crea una pipeline para cada modelo\n",
    "\n",
    "# Random Forest a esta no se le aplica un scaler ya que al ser un modelo de arboles por steps todas las variables se reciben como independientes\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"rf\", RandomForestClassifier(bootstrap=True, criterion='entropy', max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50, random_state=42))\n",
    "])\n",
    "\n",
    "# Logistic Regression estamos utilizando los parametros obtenidos en las pruebas individuales de este modelo\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(penalty='elasticnet', C=1.0, solver='saga', l1_ratio=0.5, max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBClassifier(alpha=0.1, gamma=0.5, max_depth=2, min_child_weight=1, random_state=42))\n",
    "])\n",
    "\n",
    "# Se crea un Voting Classifier con los modelos anteriores\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    (\"rf\", rf_pipeline),\n",
    "    (\"lr\", lr_pipeline),\n",
    "    (\"xgb\", xgb_pipeline)\n",
    "], voting=\"soft\")\n",
    "\n",
    "# Se entrena el Voting Classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "testing_accuracy = accuracy_score(y_test, y_pred)\n",
    "training_accuracy = accuracy_score(y_train, voting_classifier.predict(X_train))\n",
    "\n",
    "print('Voting Classifier: {:.3f}'.format(testing_accuracy))\n",
    "print('Voting Classifier: {:.3f}'.format(training_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T01:18:22.379947Z",
     "start_time": "2023-08-28T01:18:22.245355200Z"
    }
   },
   "id": "af20207c9e3f3ff2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Despues de entrenar al voting classifier podemos ver que tiene mas overfitting que el modelo de logistic regression sin embargo el modelo de Logisitc Regression tiene cambios muy altos entre cada iteracion si dejamos que sea aleatorio, por lo que sacrificaremos un poco de overfitting para tener un modelo mas estable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "109a2aa73f733861"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
